The 'aligner' directory contains several attempts at word alignment.

The usage is just like the dice.py program:

python [program_name] -n [num_lines] -i [num_iterations]

The output is the same format as the dice.py output, namely (i, j) pairs of foreign indices to English indices, one sentence per line.

-------------------------

IBM Model 1 (ibm1.py)
- Implemented very much like in the textbook.
- Depends on NLTK (Natural Language Toolkit).
- My one addition is, when determining how an English word e_j should be aligned, there are often ties among the probabilities. When this occurs, I choose the foreign word f_i such that (i / fl) is closest to (j / el), where fl and el are the lengths of the foreign sentence and English sentence, respectively.
- I didn't really know of the best convergence criterion, but I typically measured the maximum change in any probability entry of the parameters. This number would often jump around, sometimes getting smaller and seeming to converge, but then climbing back up again. Overall it tended to decrease steadily though.
- I trained the EM algorithm on just the first 1000 lines for 50 iterations, and aligned the first 1000 lines.
Precision = 0.611195
Recall = 0.710059
AER = 0.355355

IBM Model 2 (ibm2.py)
- Again, much like in the textbook, with the same modification made in IBM Model 1 for breaking ties.
- If I had had more time, I would've liked to collapse words to lower-case, since case is basically irrelevant.
- Ran it for 100 iterations:
Precision = 0.549168
Recall = 0.674556
AER = 0.408408
- It seems to do worse, but I think that's because it just needs to be trained for more iterations, due to the much larger parameter space.

HMM (hmm.py)
- This was kind of a mess and is still a work in progress. The algorithm runs and seems to produce increasingly accurate probability distributions, but it still doesn't perform too well, due to either a bug or some other consideration. 
- I followed the procedures given in the Wikipedia article for the Baum-Welch algorithm, and I mostly use the notation in that. (Also see attached jpg 'hmm.jpg', for my mathematical notes).
- For output alignments, has two options: 1) best_gamma_sequence: computes state probabilities for each English index, then chooses the maximum, 2) viterbi: computes the most likely sequence of states using the Viterbi algorithm.
- The main thing that complicated the algorithm was using the parameter s(i' - i), which is a distribution on jump distances in hidden state space, instead of the raw state transition probabiity matrix. Doing this has the advantage of reducing the size of parameter space, allowing for better performance under sparse data, but it significantly complicates the normalization procedures. I tried my best to get it right, but I'm still not 100% sure I did. In particular, I feel like the jump transition probabilities are weighted too heavily towards jumps of 0, when I would expect there should be more weight on 1 (i.e. moving forward instead of staying still).

-------------------------

- Overall, I learned a lot from this exercise. Most importanly, I realized how difficult it can be to code up algorithms whose theory is relatively straightforward. This is because of countless issues involving normalization and numerical underflow, which necessitate the frequent conversion back and forth from probabilities to log-probabilities.

- My handle on the leaderboard is anonymouse.


